{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up the environment:\n",
    "\n",
    "(1) pip install jupyterlab==2.2.5\n",
    "\n",
    "(2) nodejs:\n",
    "sudo apt update\n",
    "curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n",
    "sudo apt install nodejs\n",
    "node -v\n",
    "\n",
    "(3) npm:\n",
    "sudo apt-get install nodejs-dev node-gyp libssl1.0-dev\n",
    "sudo apt-get install npm\n",
    "\n",
    "If you are on MacOS: install both nodejs and npm from https://nodejs.org.en\n",
    "\n",
    "(4) Install the extensions \"toc\" and \"collapsible_headings\" \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import itertools\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore') \n",
    "\n",
    "import seaborn as sns # If you got the error \"No module named 'numpy.testing.decorators'\", downgrade numpy to 1.17.0\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# If you encounter errors when installing xgboost:\n",
    "# try to install libomp by \"brew install libomp\" on MacOS or install cmake by \"pip3 install cmake\"\n",
    "# pip install xgboost==1.5.2\n",
    "import xgboost as xgb\n",
    "\n",
    "# pip install scikit-learn==0.24.2\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import (\n",
    "    mutual_info_score, confusion_matrix, \n",
    "    accuracy_score, recall_score, precision_score, \n",
    "    precision_recall_fscore_support, make_scorer, \n",
    "    plot_roc_curve, plot_precision_recall_curve, \n",
    "    classification_report, roc_auc_score, mean_squared_error\n",
    ")\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def get_clf_metrics(y_true, y_pred, n_labels):\n",
    "    result = pd.DataFrame(index=list(range(n_labels)), columns=['accuracy', 'precision', 'recall', 'f-score'])\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    array = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    for i in range(n_labels):\n",
    "        result.loc[i, 'accuracy'] = acc\n",
    "        result.loc[i, 'precision'] = array[0][i]\n",
    "        result.loc[i, 'recall'] = array[1][i]\n",
    "        result.loc[i, 'f-score'] = array[2][i]\n",
    "    return result\n",
    "\n",
    "def get_confusion_matrix(y_true, y_pred, n_labels):\n",
    "    result = pd.DataFrame(confusion_matrix(y_true, y_pred), index=list(range(n_labels)), columns=list(range(n_labels)))\n",
    "    result.index.name = 'true'\n",
    "    result.columns.name = 'pred'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good datasets you can try: https://towardsdatascience.com/all-the-datasets-you-need-to-practice-data-science-skills-and-make-a-great-portfolio-74f2eb53b38a\n",
    "df = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/home_data.csv', sep=',', index_col=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the data types for some columns\n",
    "# It is a good practice to define several lists of columns\n",
    "\n",
    "\"\"\"\n",
    "zipcode is usually a very important variable, but it is a categorical variable with too many categories, \n",
    "so we usually need to do \"binning\", e.g., group them according to the first three digits, \n",
    "but here for simplicity, we just ignore it\n",
    "\n",
    "yr_renovated has many zeros (zero inflated) and few large numbers (years), \n",
    "so we usually should make it a categorical variable with three categories, e.g., (1) \"Never renovated\", (2) \"Renovated within 5 years\", (3) \"Renovated more than 5 years ago\",\n",
    "but here for simplicity, we just ignore it\n",
    "\"\"\"\n",
    "ignore_cols = ['id', 'zipcode', 'yr_renovated']\n",
    "\n",
    "dt_cols = ['date']\n",
    "for col in dt_cols:\n",
    "    df[col] = pd.to_datetime(df[col])  # You can't directly do this in the above \"dtype\"\n",
    "    \n",
    "res_cols = ['price']\n",
    "df['price'] = pd.Categorical((df['price'] > df['price'].mean()).astype(int), ordered=True) # We make it a binary classification problem\n",
    "\n",
    "cat_cols = ['waterfront', 'view', 'condition', 'grade']\n",
    "for col in cat_cols:\n",
    "    df[col] = pd.Categorical(df[col], ordered=True)  # To keep the order of categorical variables, we set ordered = True\n",
    "    \n",
    "cont_cols = [col for col in df.columns if col not in ignore_cols + dt_cols + res_cols + cat_cols]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy()\n",
    "\n",
    "# Manually create some missing values\n",
    "np.random.seed(0)\n",
    "df.loc[np.random.choice(df.index, 10), 'sqft_living'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 20), 'sqft_above'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 5), 'grade'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30), 'waterfront'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the columns have the correct data types and check whether there are missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for each individual variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize=(40, 35))\n",
    "\n",
    "fig.suptitle('Plots for each individual variable')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(res_cols + cat_cols + cont_cols):\n",
    "    if df[col].dtype.name == 'category':\n",
    "        sns.countplot(ax=axes[i], x=col, data=df)\n",
    "    else:\n",
    "        sns.histplot(ax=axes[i], x=col, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for each pair of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Good reference: \n",
    "https://seaborn.pydata.org/tutorial/categorical.html\n",
    "https://data-science-master.github.io/lectures/09_python/09_matplotlib.html\n",
    "https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "\n",
    "    Variable on Y                    Variable on X                   Plot\n",
    "(1) continuous (int or float)        continuous (int or float)       scatter plot / regplot with 95% CI\n",
    "(2) continuous (int or float)        category (ordered or not)       swarmplot / stripplot / violinplot / boxplot \n",
    "(3) category (ordered or not)        continuous (int or float)       swarmplot / stripplot / violinplot / boxplot / histplot with the categorical variable as hue\n",
    "(4) category (ordered or not)        category (ordered or not)       confusion table / bar plot with hue (bar height is the count)\n",
    "\n",
    "You may also consider to incorporate some numeric characteristics (e.g., mean, median):\n",
    "\n",
    "For (1), you can also use lineplot, where x is one variable and y is some characteristic of the other variable\n",
    "\n",
    "In seaborn, lineplot has an argument \"estimator\" for aggregating across multiple observations of the variable on Y at the same X level. \n",
    "If it is None, all observations will be drawn\n",
    "\n",
    "For (2) (3) (4), you can also use bar plot, where each class of a categorical variable corresponds to a bar and \n",
    "the bar height / length is some characteristic of the other variable (you can specify the \"estimator\" argument in barplot)\n",
    "\n",
    "In seaborn, the barplot() function operates on a full dataset and applies a function to obtain the estimate (taking the mean by default). \n",
    "When there are multiple observations in each category, it also uses bootstrapping to compute a confidence interval around the estimate, which is plotted using error bars\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 7))\n",
    "\n",
    "# Case (1) scatter plot\n",
    "sns.scatterplot(ax=axes[0], x='sqft_above', y='sqft_living', data=df)\n",
    "\n",
    "# Case (1) regplot\n",
    "sns.regplot(ax=axes[1], x='sqft_above', y='sqft_living', data=df)\n",
    "\n",
    "# Case (2) lineplot\n",
    "sns.lineplot(ax=axes[2], x='sqft_above', y='sqft_living', data=df, estimator=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 6, figsize=(60, 7))\n",
    "\n",
    "# Case (2) swarmplot\n",
    "sns.swarmplot(ax=axes[0], x='grade', y='sqft_above', data=df.iloc[:100, :])  # swarmplot can't show too many points, so we select only the first 100 points\n",
    "\n",
    "# Case (2) stripplot\n",
    "sns.stripplot(ax=axes[1], x='grade', y='sqft_above', data=df)\n",
    "\n",
    "# Case (2) violinplot\n",
    "sns.violinplot(ax=axes[2], x='grade', y='sqft_above', data=df)\n",
    "\n",
    "# Case (2) boxplot\n",
    "sns.boxplot(ax=axes[3], x='grade', y='sqft_above', data=df)\n",
    "\n",
    "# Case (2) barplot with mean of sqft_above as bar height\n",
    "sns.barplot(ax=axes[4], x='grade', y='sqft_above', data=df, estimator=np.mean)\n",
    "\n",
    "# Case (2) barplot with median of sqft_above as bar height\n",
    "sns.barplot(ax=axes[5], x='grade', y='sqft_above', data=df, estimator=np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 7, figsize=(70, 7))\n",
    "\n",
    "# Case (3) swarmplot\n",
    "sns.swarmplot(ax=axes[0], x='sqft_above', y='grade', data=df.iloc[:100, :])  # swarmplot can't show too many points, so we select only the first 100 points\n",
    "\n",
    "# Case (3) stripplot\n",
    "sns.stripplot(ax=axes[1], x='sqft_above', y='grade', data=df)\n",
    "\n",
    "# Case (3) violinplot\n",
    "sns.violinplot(ax=axes[2], x='sqft_above', y='grade', data=df)\n",
    "\n",
    "# Case (3) boxplot\n",
    "sns.boxplot(ax=axes[3], x='sqft_above', y='grade', data=df)\n",
    "\n",
    "# Case (3) histplot with the categorical variable as hue\n",
    "sns.histplot(ax=axes[4], x='sqft_above', hue='grade', data=df)\n",
    "\n",
    "# Case (3) barplot with mean of sqft_above as bar length\n",
    "sns.barplot(ax=axes[5], x='sqft_above', y='grade', data=df, estimator=np.mean)\n",
    "\n",
    "# Case (3) barplot with median of sqft_above as bar length\n",
    "sns.barplot(ax=axes[6], x='sqft_above', y='grade', data=df, estimator=np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 7))\n",
    "\n",
    "# Case (4) confusion table\n",
    "confusion_table = pd.crosstab(df['grade'], df['condition'], rownames=['grade'], colnames=['condition'])\n",
    "# confusion_table = pd.crosstab(df['grade'], [df['condition'], df['view']], rownames=['grade'], colnames=['condition', 'view'], margins=True)\n",
    "sns.heatmap(ax=axes[0], data=confusion_table, cmap='Reds', annot=True)\n",
    "\n",
    "# Case (4) bar plot with hue (bar height is the count)\n",
    "sns.countplot(ax=axes[1], x='grade', hue='condition', data=df)\n",
    "\n",
    "# Case (4) barplot with mean of waterfront (rate of waterfront) as bar height\n",
    "df_copy = df.copy()\n",
    "df_copy['waterfront'] = df_copy['waterfront'].astype('float')  # We must make waterfront to be numeric, otherwise we can't calculate its mean\n",
    "sns.barplot(ax=axes[2], x='grade', y='waterfront', data=df_copy, estimator=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for each individual variable and each pair of variables simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Good reference: \n",
    "https://seaborn.pydata.org/generated/seaborn.PairGrid.html\n",
    "https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "\n",
    "We always recommend to separate into (1) cont vs cont (2) cat vs cont (3) cont vs cat (4) cat vs cat\n",
    "as we need different kinds of plots for each of these four cases\n",
    "\n",
    "In general you have the following choices:\n",
    "\n",
    "Response    How to regard response in plot         Types of other variables\n",
    "---------------------------------------------------------------------------\n",
    "cat         hue                                    cont vs cont\n",
    "cat         hue                                    cat vs cont\n",
    "cat         hue                                    cont vs cat\n",
    "cat         hue                                    cat vs cat\n",
    "---------------------------------------------------------------------------\n",
    "cat         cat (plotted only when there is cat)   cont vs cont\n",
    "cat         cat (plotted only when there is cat)   cat vs cont\n",
    "cat         cat (plotted only when there is cat)   cont vs cat\n",
    "cat         cat (plotted only when there is cat)   cat vs cat\n",
    "---------------------------------------------------------------------------\n",
    "cont        hue                                    cont vs cont\n",
    "cont        hue                                    cat vs cont\n",
    "cont        hue                                    cont vs cat\n",
    "cont        hue                                    cat vs cat\n",
    "---------------------------------------------------------------------------\n",
    "cont        cont (plotted only when there is cont) cont vs cont\n",
    "cont        cont (plotted only when there is cont) cat vs cont\n",
    "cont        cont (plotted only when there is cont) cont vs cat\n",
    "cont        cont (plotted only when there is cont) cat vs cat\n",
    "\n",
    "Here we only consder the first four cases\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (5, 5)})\n",
    "\n",
    "# Randomly sample 100 points to increase plotting speed\n",
    "# Or a better way is to randomly sample 10% of the data and plot\n",
    "df_curr = df.sample(frac=1, random_state=0).iloc[:100, :]\n",
    "\n",
    "g = sns.PairGrid(df_curr, x_vars=cont_cols, y_vars=cont_cols, hue='price')\n",
    "# g = sns.PairGrid(df_curr, x_vars=cont_cols, y_vars=cont_cols)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_upper(sns.regplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (5, 5)})\n",
    "\n",
    "# Randomly sample 100 points to increase plotting speed\n",
    "df_curr = df.sample(frac=1, random_state=0).iloc[:100, :]\n",
    "\n",
    "g = sns.PairGrid(df_curr, x_vars=cat_cols, y_vars=cont_cols, hue='price')\n",
    "g.map_offdiag(sns.boxplot)  # There is no \"diag\" when x_vars != y_vars!\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (5, 5)})\n",
    "\n",
    "# Randomly sample 100 points to increase plotting speed\n",
    "df_curr = df.sample(frac=1, random_state=0).iloc[:100, :]\n",
    "\n",
    "g = sns.PairGrid(df_curr, x_vars=cat_cols, y_vars=cat_cols, hue='price')\n",
    "g.map_diag(sns.countplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (5, 5)})\n",
    "\n",
    "# Randomly sample 100 points to increase plotting speed\n",
    "df_curr = df.sample(frac=1, random_state=0).iloc[:100, :]\n",
    "\n",
    "# pairplot is a high-level interface for PairGrid that is intended to make it easy to draw a few common styles\n",
    "# You should use PairGrid directly if you need more flexibility.\n",
    "# It will simply skip the categorical variables, which is bad!\n",
    "sns.pairplot(df_curr, hue='price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations between every pair of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Variable1                        Variable2                       Correlation\n",
    "(1) continuous (int or float)        continuous (int or float)       Pearson's correlation (ss.pearsonr(cont1, cont2) or you can directly use df.corr())\n",
    "(2) continuous (int or float)        category (ordered or not)       Kruskal-Wallis H-test (kruskal_wallis(cat1, cont2))\n",
    "(3) category (ordered or not)        category (ordered or not)       Chi square statistic (chi_square(cat1, cat2)) / mutual information (mutual_info_score(cat1, cat2))\n",
    "\n",
    "If you observe very high correlation between some features, consider to drop one of these highly correlated features \n",
    "\"\"\"\n",
    "\n",
    "def chi_square(cat1, cat2, use_pvalue=True):\n",
    "    confusion_table = pd.crosstab(cat1, cat2)\n",
    "    return ss.chi2_contingency(confusion_table)[1] if use_pvalue else ss.chi2_contingency(confusion_table)[0]\n",
    "\n",
    "def kruskal_wallis(cat1, cont2, use_pvalue=True):\n",
    "    temp = pd.concat((cat1, cont2), axis=1)\n",
    "    samples = tuple(temp.loc[temp[cat1.name] == val, cont2.name].tolist() for val in cat1.unique())\n",
    "    return ss.kruskal(*samples)[1] if use_pvalue else ss.kruskal(*samples)[0]\n",
    "\n",
    "def get_corr(df, cols1, cols2):        \n",
    "    result = pd.DataFrame(index=cols1, columns=cols2)\n",
    "    for col1 in cols1: \n",
    "        for col2 in cols2:\n",
    "            mask = (df[col1].notnull() & df[col2].notnull())\n",
    "            c1, c2 = df.loc[mask, col1], df.loc[mask, col2]\n",
    "            is_cat1, is_cat2 = (c1.dtype.name == 'category'), (c2.dtype.name == 'category')\n",
    "            if is_cat1 and is_cat2:\n",
    "                # result.loc[col1, col2] = mutual_info_score(c1, c2)\n",
    "                result.loc[col1, col2] = chi_square(c1, c2)\n",
    "            elif is_cat1 and not is_cat2:\n",
    "                result.loc[col1, col2] = kruskal_wallis(c1, c2)\n",
    "            elif not is_cat1 and is_cat2:\n",
    "                result.loc[col1, col2] = kruskal_wallis(c2, c1)\n",
    "            else:\n",
    "                result.loc[col1, col2] = ss.pearsonr(c1, c2)[0]\n",
    "    return result.astype('float').round(5)  # Must make them float to use sns.heatmap; otherwise they are np.float64, which are not accepted by sns.heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "\n",
    "cont_cont_corr = get_corr(df, cont_cols, cont_cols)\n",
    "# cont_cont_corr should be the same as df.corr(), as df.corr() will skip / ignore any non-continuous variables\n",
    "assert (cont_cont_corr - df.corr()).abs().max().max() < 1e-5\n",
    "sns.heatmap(cont_cont_corr, cmap='Reds', annot=True)\n",
    "cont_cont_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "\n",
    "cat_cont_corr = get_corr(df, cat_cols, cont_cols)\n",
    "sns.heatmap(cat_cont_corr, cmap='Reds', annot=True)\n",
    "cat_cont_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "\n",
    "cat_cat_corr = get_corr(df, cat_cols, cat_cols)\n",
    "sns.heatmap(cat_cat_corr, cmap='Reds', annot=True)\n",
    "cat_cat_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check whether there is duplication to avoid leaking data to the testing set\n",
    "df.loc[df.duplicated(keep=False), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df.loc[:, cat_cols + cont_cols]\n",
    "y_df = df.loc[:, res_cols]\n",
    "\n",
    "# Use stratify to make sure the label distributions in training and testing sets are the same\n",
    "# See https://gist.github.com/SHi-ON/63839f3a3647051a180cb03af0f7d0d9\n",
    "# We use the while loop to ensure all categories of all categorical varialbes occur in the training set\n",
    "seed = -1\n",
    "while seed == -1 or any(x_train[col].nunique() != df[col].nunique() for col in cat_cols):\n",
    "    seed += 1\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.3, random_state=seed, stratify=y_df)\n",
    "    \n",
    "print(seed)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection and handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider each individual dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, you should understand we only need to handle outliers for continuous variables\n",
    "\n",
    "The main method to detect outliers for each individual dimension is to plot boxplot, histplot and kdeplot\n",
    "\n",
    "You may observe three patterns when you look at the histplot:\n",
    "\n",
    "(1) There is a peak on one side, there are very few values far away from the peak, and there is no density in the middle \n",
    "    -> The far-away points are outliers \n",
    "    -> You can \n",
    "       1) discard them\n",
    "       2) truncate them: for any values out of 2.5% quantile (or Q1 - 1.5IQR) and 97.5% quantile (or Q3 + 1.5IQR), we let them be 2.5% quantile (or Q1 - 1.5IQR) or 97.5% quantile (or Q3 + 1.5IQR)\n",
    "       3) let them be NA, which will be imputed in the following missing value imputation step\n",
    "    -> You may also consider to tell engineers there may be something wrong with their logging code\n",
    "       \n",
    "(2) There is a peak on one side, there are very few values far away from the peak, but there is some density in the middle \n",
    "    -> The distribution is highly skewed (in this case, you shouldn't discard or truncate the values; otherwise you lose lots of information)\n",
    "    -> You can\n",
    "       1) do the log transformation if all values > 0: x -> log(x)\n",
    "       2) do the plus1-log transformation if all values >= 0: x -> log(x + 1) (0 will still be converted to 0)\n",
    "       3) make x be x ^ (1 / 3) if some values < 0\n",
    "\n",
    "(3) There is a peak on one side, there are lots of values far away from the peak (i.e., it is another peak)\n",
    "    -> The distribution is multi-mode (in this case, you shouldn't discard or truncate the values; otherwise you lose lots of information)\n",
    "    -> There is nothing you can do. Just leave it as it is\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(cont_cols), 3, figsize=(30, len(cont_cols) * 7))\n",
    "\n",
    "fig.suptitle('Plots to detect outliers for each individual continuous variable')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "for i, col in enumerate(cont_cols):\n",
    "    sns.boxplot(ax=axes[i][0], x=col, data=x_train)\n",
    "    sns.histplot(ax=axes[i][1], x=col, data=x_train)\n",
    "    sns.kdeplot(ax=axes[i][2], x=col, data=x_train, common_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.quora.com/Should-we-exclude-outliers-form-testing-set\n",
    "If you are trying to test how good your model does with outliers in your data (maybe in real-world you wonâ€™t be able to curate the data to reduce outliers) then you should include the outliers in your test data. \n",
    "But, if you are confident that you would be able to filter outliers before it comes to your machine learning model for prediction, there is no point having the outliers in your test data.\n",
    "\"\"\"\n",
    "\n",
    "class OutlierClipper:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        self.bounds = dict()\n",
    "        \n",
    "    def fit(self, df):\n",
    "        assert len(self.bounds) == 0\n",
    "        \n",
    "        # Make sure all columns of df are continuous variables\n",
    "        for col in df.columns:\n",
    "            if self.method == 'iqr':\n",
    "                # See https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/\n",
    "                q25 = df[col].quantile(0.25)\n",
    "                q75 = df[col].quantile(0.75)\n",
    "                iqr = q75 - q25\n",
    "                lb = q25 - 1.5 * iqr\n",
    "                ub = q75 + 1.5 * iqr\n",
    "            elif type(self.method) == tuple:\n",
    "                # Winsorizing: https://en.wikipedia.org/wiki/Winsorizing\n",
    "                lb = df[col].quantile(self.method[0])\n",
    "                ub = df[col].quantile(self.method[1])\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "            self.bounds[col] = (lb, ub)\n",
    "            \n",
    "    def transform(self, df):\n",
    "        assert len(self.bounds) != 0 and sorted(self.bounds.keys()) == sorted(df.columns)\n",
    "        \n",
    "        df_result = df.copy()\n",
    "        for col in df.columns:\n",
    "            lb, ub = self.bounds[col]\n",
    "            df_result[col] = df[col].clip(lb, ub)\n",
    "            print('{0:d} outliers for {1:} clipped'.format((~df[col].between(lb, ub)).sum(), col))\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "# The large values in these two columns actually are not large enough to be regarded as \"outliers\"\n",
    "# If the large values are like \"100\", then they can be regarded as outliers\n",
    "# Here, since we want to try the OutlierClipper, we simply regard them as outliers\n",
    "clip_cols = ['bedrooms', 'bathrooms']\n",
    "\n",
    "outlier_clipper = OutlierClipper(method=(0.005, 0.995))\n",
    "outlier_clipper.fit(x_train.loc[:, clip_cols])\n",
    "x_train.loc[:, clip_cols] = outlier_clipper.transform(x_train.loc[:, clip_cols])\n",
    "x_test.loc[:, clip_cols] = outlier_clipper.transform(x_test.loc[:, clip_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformer:\n",
    "    def __init__(self):\n",
    "        self.min_val = dict()\n",
    "        \n",
    "    def fit(self, df):\n",
    "        assert len(self.min_val) == 0        \n",
    "        self.min_val = {col: df[col].min() for col in df.columns}\n",
    "\n",
    "    def transform(self, df):\n",
    "        assert len(self.min_val) != 0 and sorted(self.min_val.keys()) == sorted(df.columns)\n",
    "        \n",
    "        df_result = df.copy()\n",
    "        for col in df.columns:\n",
    "            min_val = self.min_val[col]\n",
    "            if min_val <= 0:\n",
    "                df_result[col] = df[col].map(lambda x: np.log(x - min_val + 1))\n",
    "            else:\n",
    "                df_result[col] = df[col].map(lambda x: np.log(x))\n",
    "        \n",
    "        return df_result\n",
    "\n",
    "log_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15']\n",
    "log_transformer = LogTransformer()\n",
    "log_transformer.fit(x_train.loc[:, log_cols])\n",
    "x_train.loc[:, log_cols] = log_transformer.transform(x_train.loc[:, log_cols])\n",
    "x_test.loc[:, log_cols] = log_transformer.transform(x_test.loc[:, log_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(cont_cols), 3, figsize=(30, len(cont_cols) * 7))\n",
    "\n",
    "fig.suptitle('Plots for each individual continuous variable after handling outliers')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "for i, col in enumerate(cont_cols):\n",
    "    sns.boxplot(ax=axes[i][0], x=col, data=x_train)\n",
    "    sns.histplot(ax=axes[i][1], x=col, data=x_train)\n",
    "    sns.kdeplot(ax=axes[i][2], x=col, data=x_train, common_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider all dimensions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "# See https://github.com/yzhao062/pyod\n",
    "\n",
    "df_curr = x_train.loc[x_train.notnull().all(axis=1), cont_cols]  # The following outlier detectors can't handle nan values, so we remove all lines with >= 1 nan\n",
    "\n",
    "outlier_detector = LocalOutlierFactor(n_neighbors=20)\n",
    "outlier_detector.fit_predict(df_curr)\n",
    "\n",
    "# A negative_outlier_factor_ close to -1 means a inliers, while a smaller negative_outlier_factor_ means an outlier\n",
    "outlier_score = pd.Series(outlier_detector.negative_outlier_factor_, index=df_curr.index)\n",
    "outlier_score.name = 'outlier_score'\n",
    "temp = pd.concat((df_curr, outlier_score), axis=1)\n",
    "\n",
    "sns.histplot(x='outlier_score', data=temp)\n",
    "\n",
    "# According to the histogram, the samples with negative_outlier_factor_ <= -2 could be outliers\n",
    "temp.loc[temp['outlier_score'] <= -2, :].sort_values(by=['outlier_score', 'sqft_living'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curr = x_train.loc[x_train.notnull().all(axis=1), cont_cols]\n",
    "\n",
    "outlier_detector = IsolationForest(random_state=0)\n",
    "outlier_label = outlier_detector.fit_predict(df_curr)\n",
    "\n",
    "outlier_label = pd.Series(outlier_label, index=df_curr.index)\n",
    "outlier_label.name = 'outlier_label'\n",
    "temp = pd.concat((df_curr, outlier_label), axis=1)\n",
    "\n",
    "temp.loc[temp['outlier_label'] == -1, :].sort_values(by='sqft_living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here you may want to ask them why these data are missing\n",
    "\n",
    "(1) The data could be Missing Completely at Random (MCAR), i.e., the events that lead to the value being missing doesn't depend on observed or unobserved values\n",
    "(2) The data could be Missing at Random (MAR), i.e., the events that lead to the value being missing depends only on observed values\n",
    "(3) The data could be Missing not at Random (MNAR), i.e., the events that lead to the value being missing depends on unobserved values \n",
    "    (e.g., Tom's exam score is missing because he hides the exam solution book, as he got a low score and he doesn't want other people to know it, i.e., \n",
    "     the reason it is missing is related to the unobserved value of his exam score)\n",
    "     \n",
    "    Another example of MNAR. When you do random sampling, some item is not observed (its count is zero), \n",
    "    because the total count of this item in the population is too small and a simple random sampling can't capture it\n",
    "    In this case, we may impute its count by \"min count of the observed items / sqrt(2)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "# Univariate feature imputation for categorical variables\n",
    "\n",
    "# We must convert categorical variables to float first, otherwise SimpleImputer can't correctly handle it\n",
    "for col in cat_cols:\n",
    "    x_train[col] = x_train[col].astype(float)\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "cat_imp.fit(x_train.loc[:, cat_cols])\n",
    "x_train.loc[:, cat_cols] = cat_imp.transform(x_train.loc[:, cat_cols])\n",
    "for col in cat_cols:\n",
    "    x_train[col] = pd.Categorical(x_train[col], ordered=True)\n",
    "    \n",
    "for col in cat_cols:\n",
    "    x_test[col] = x_test[col].astype(float)\n",
    "x_test.loc[:, cat_cols] = cat_imp.transform(x_test.loc[:, cat_cols])\n",
    "for col in cat_cols:\n",
    "    x_test[col] = pd.Categorical(x_test[col], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate feature imputation for continuous variables\n",
    "\n",
    "cont_imp = SimpleImputer(strategy='mean')\n",
    "cont_imp.fit(x_train.loc[:, cont_cols])\n",
    "x_train.loc[:, cont_cols] = cont_imp.transform(x_train.loc[:, cont_cols])\n",
    "x_test.loc[:, cont_cols] = cont_imp.transform(x_test.loc[:, cont_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate feature imputation for continuous variables\n",
    "\n",
    "# Can only be used for continuous variables\n",
    "iter_imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "iter_imp.fit(x_train.loc[:, cont_cols])\n",
    "x_train.loc[:, cont_cols] = iter_imp.transform(x_train.loc[:, cont_cols])\n",
    "x_test.loc[:, cont_cols] = iter_imp.transform(x_test.loc[:, cont_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(x_train.loc[:, cont_cols])\n",
    "x_train.loc[:, cont_cols] = scaler.transform(x_train.loc[:, cont_cols])\n",
    "x_test.loc[:, cont_cols] = scaler.transform(x_test.loc[:, cont_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(cont_cols), 3, figsize=(30, len(cont_cols) * 7))\n",
    "\n",
    "fig.suptitle('Plots for each individual continuous variable after handling outliers')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "for i, col in enumerate(cont_cols):\n",
    "    sns.boxplot(ax=axes[i][0], x=col, data=x_train)\n",
    "    sns.histplot(ax=axes[i][1], x=col, data=x_train)\n",
    "    sns.kdeplot(ax=axes[i][2], x=col, data=x_train, common_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We append x_test to x_train and call pd.get_dummies, and then we separate them back\n",
    "# We use this trick to handle the cases: (1) there are categories that are in x_train but are not in x_test; (2) there are categories that are in x_test but are not in x_train\n",
    "# Without this trick, the one-hot vectors in x_train and x_test could mismatch\n",
    "\n",
    "assert (x_train.columns == x_test.columns).all()\n",
    "x_train_test = x_train.append(x_test)\n",
    "x_train_test_dummies = pd.get_dummies(x_train_test, columns=cat_cols, drop_first=True)\n",
    "\n",
    "x_train = x_train_test_dummies.iloc[:x_train.shape[0], :]\n",
    "x_test = x_train_test_dummies.iloc[x_train.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_train.loc[:, cont_cols])\n",
    "x_train_pca = pd.DataFrame(pca.transform(x_train.loc[:, cont_cols]))\n",
    "\n",
    "sns.scatterplot(x=0, y=1, data=x_train_pca)\n",
    "# for i in x_train_pca.index:\n",
    "#     plt.annotate(i, (x_train_pca.loc[i, 0] + 0.7, x_train_pca.loc[i, 1] + 0.7) )\n",
    "    \n",
    "x_train_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression + binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "\"\"\"\n",
    "'newton-cg' - ['l2', 'none']\n",
    "'lbfgs' - ['l2', 'none']\n",
    "'liblinear' - ['l1', 'l2']\n",
    "'sag' - ['l2', 'none']\n",
    "'saga' - ['elasticnet', 'l1', 'l2', 'none']\n",
    "\"\"\"\n",
    "\n",
    "model = LogisticRegression(penalty='l1', C=1.0, fit_intercept=True, class_weight=None, \n",
    "                           random_state=0, solver='liblinear', max_iter=100, multi_class='ovr').fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty': ['l1', 'l2'], 'solver': ['liblinear'], 'C': [100, 10, 5, 2, 1], 'class_weight': [{0: 1.2, 1: 1.0}, 'balanced']},\n",
    "    {'penalty': ['none'], 'solver': ['newton-cg'], 'C': [100, 10, 5, 2, 1], 'class_weight': [{0: 1.2, 1: 1.0}, 'balanced']}\n",
    "]\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    LogisticRegression(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1\n",
    ")\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "# rs = RandomizedSearchCV(\n",
    "#     LogisticRegression(random_state=0),\n",
    "#     param_distributions=param_grid,\n",
    "#     n_iter=15,\n",
    "#     scoring=scoring,\n",
    "#     refit='AUC',\n",
    "#     return_train_score=False,\n",
    "#     cv=5, n_jobs=-1, random_state=0\n",
    "# )\n",
    "# rs.fit(x_train, y_train)\n",
    "\n",
    "model = gs.best_estimator_\n",
    "print(gs.best_estimator_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "result = gs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest + binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               criterion='gini',\n",
    "                               max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                               max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                               ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                               min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                               min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                               min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               max_features='sqrt', # Make sure trees are independent\n",
    "                               bootstrap=True, # Make sure trees are independent\n",
    "                               max_samples=None, # Make sure trees are independent\n",
    "                               oob_score=True,\n",
    "                               class_weight='balanced',\n",
    "                               n_jobs=-1,\n",
    "                               random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "tree.plot_tree(model.estimators_[0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [100, 200], \n",
    "     'criterion': ['gini', 'entropy'], \n",
    "     'max_depth': [3, 6], \n",
    "     'min_samples_split': [2, 10],\n",
    "     'min_samples_leaf': [1, 5],\n",
    "     'min_impurity_decrease': [0, 0.02],\n",
    "     'class_weight': [{0: 1.2, 1: 1.0}, 'balanced']}\n",
    "]\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     RandomForestClassifier(random_state=0),\n",
    "#     param_grid=param_grid,\n",
    "#     scoring=scoring,\n",
    "#     refit='AUC',\n",
    "#     return_train_score=False,\n",
    "#     cv=5, n_jobs=-1\n",
    "# )\n",
    "# gs.fit(x_train, y_train)\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT + binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To understand how to use GBDT for classification, see\n",
    "https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full\n",
    "https://stats.stackexchange.com/questions/204154/classification-with-gradient-boosting-how-to-keep-the-prediction-in-0-1\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "For binary classification, GBDT is trying to use a regression tree to model the log odds ratio (the \"linear\" part), instead of the observed 0, 1\n",
    "For multiple classification with K classes, GBDT will build K trees at the same time to model the \"linear\" part of the probability (created by softmax) of each class \n",
    "\"\"\"\n",
    "\n",
    "# Train the model once\n",
    "model = GradientBoostingClassifier(loss='deviance',  # binomial or multinomial deviance loss function\n",
    "                                   learning_rate=0.1, # Learning rate shrinks the contribution of each tree by learning_rate. The larger the learning rate is, the fewer estimators we need\n",
    "                                   n_estimators=100, # Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance\n",
    "                                   criterion='friedman_mse',\n",
    "                                   init='zero', # An estimator object that is used to compute the initial predictions\n",
    "                                   max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                                   max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                                   ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                                   min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                                   min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                                   min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                                   min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                                   subsample=1.0,  # The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias\n",
    "                                   max_features=None, # Make sure trees are independent\n",
    "                                   random_state=0).fit(x_train, y_train)  # Unlike Random Forest, there is no \"class_weight\" in model, we only have \"sample_weight\" in fit (see https://scikit-learn.org/0.24/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.fit)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "tree.plot_tree(model.estimators_[0][0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest, where now model.estimators_ is an array of size n_estimators * loss_.K\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'learning_rate': [0.01, 0.1],\n",
    "     'n_estimators': [50, 100], \n",
    "     'max_depth': [3, 6], \n",
    "     'min_samples_split': [2, 10],\n",
    "     'min_samples_leaf': [1, 5],\n",
    "     'min_impurity_decrease': [0, 0.02], \n",
    "     'subsample': [1, 0.8]}\n",
    "]\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost + binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=feature_importances#xgboost.XGBClassifier\n",
    "# https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "\"\"\"\n",
    "You may save a trained model by: \n",
    "model.save_model('/path/to/model.json')\n",
    "\n",
    "You may load the saved model by:\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model('/path/to/model.json')\n",
    "\"\"\"\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "        n_estimators=4,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.9,\n",
    "        objective='binary:logistic',\n",
    "        booster='gbtree',\n",
    "        tree_method='exact',\n",
    "        gamma=0.0,\n",
    "        min_child_weight=1.0,\n",
    "        max_delta_step=0.0,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.5,\n",
    "        random_state=0,\n",
    "        use_label_encoder=False,\n",
    "        missing=np.nan,\n",
    "        importance_type=None,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "If \"balanced\", class weight for class i = (n_total_samples / n_classes) * (1 / n_samples_of_class_i)\n",
    "If a dictionary is given, keys are classes and values are corresponding class weights\n",
    "If None is given, the class weights will be uniform\n",
    "\"\"\"\n",
    "weights = compute_class_weight('balanced', classes=y_train.unique(), y=y_train)\n",
    "dict_weights = dict(zip(y_train.unique(), weights))\n",
    "print(dict_weights)\n",
    "\n",
    "model.fit(x_train, y_train, eval_metric='error', sample_weight=y_train.replace(dict_weights))\n",
    "\n",
    "print(model.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [5, 10, 15, 20], \n",
    "     'learning_rate': [0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "     'max_depth': [3, 6, 9, 12], \n",
    "     'gamma': [0.0, 0.5, 1.0],\n",
    "     'min_child_weight': [1.0, 2.0],\n",
    "     'max_delta_step': [0.0, 0.5], \n",
    "     'lambda': [0.0, 0.5, 1.0], \n",
    "     'alpha': [0.0, 0.5, 1.0], \n",
    "    }\n",
    "]\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    xgb.XGBClassifier(use_label_encoder=False, random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "weights = compute_class_weight('balanced', classes=y_train.unique(), y=y_train)\n",
    "dict_weights = dict(zip(y_train.unique(), weights))\n",
    "rs.fit(x_train, y_train, eval_metric='error', sample_weight=y_train.replace(dict_weights))\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP + binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(30, 20, 10),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size=256,\n",
    "    learning_rate='constant', # Only used when solver='sgd'\n",
    "    learning_rate_init=0.001, # Only used when solver='sgd' or 'adam'\n",
    "    power_t=0.5, # It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'\n",
    "    max_iter=200, # For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps\n",
    "    shuffle=True, # Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'\n",
    "    random_state=0,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1 # The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "\n",
    "print(model.n_layers_)\n",
    "print(model.n_outputs_)\n",
    "print(model.out_activation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loss = pd.DataFrame({'n_epoch': list(range(1, len(model.loss_curve_) + 1)), 'train_loss': model.loss_curve_})\n",
    "sns.lineplot(x='n_epoch', y='train_loss', data=df_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'hidden_layer_sizes': list(itertools.product(*[[40, 30], [30, 20], [20, 10, 5]])), \n",
    "     'alpha': [0.001, 0.0001], \n",
    "     'batch_size': [512, 256], \n",
    "     'learning_rate_init': [0.01, 0.001]},\n",
    "]\n",
    "\"\"\"\n",
    "Candidate values of hidden_layer_sizes are\n",
    "[(40, 30, 20),\n",
    " (40, 30, 10),\n",
    " (40, 30, 5),\n",
    " (40, 20, 20),\n",
    " (40, 20, 10),\n",
    " (40, 20, 5),\n",
    " (30, 30, 20),\n",
    " (30, 30, 10),\n",
    " (30, 30, 5),\n",
    " (30, 20, 20),\n",
    " (30, 20, 10),\n",
    " (30, 20, 5)]\n",
    "\"\"\"\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    MLPClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC + binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1.0, \n",
    "            kernel='rbf',\n",
    "            degree=3, # Degree of the polynomial kernel function ('poly'). Ignored by all other kernels\n",
    "            gamma='auto', # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'; if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma; if 'auto', uses 1 / n_features\n",
    "            coef0=0.0, # Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'\n",
    "            shrinking=True, \n",
    "            probability=False, # Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation\n",
    "            class_weight='balanced',\n",
    "            random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [100, 10, 1], \n",
    "     'kernel': ['linear', 'rbf', 'poly'], \n",
    "     'class_weight': [{0: 1.2, 1: 1.0}, 'balanced']},\n",
    "]\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    SVC(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba_pred = model.predict_proba(x_test)  # predict_proba is not available when probability=False\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "# assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression + multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mul_clf_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = LogisticRegression(penalty='l1', C=1.0, fit_intercept=True, class_weight=None, \n",
    "                           random_state=0, solver='saga', tol=5e-3, max_iter=200, multi_class='multinomial').fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty': ['l1'], 'solver': ['saga'], 'C': [100, 10, 5, 2, 1], 'class_weight': [{0: 1.2, 1: 1.0, 2: 2.0}, 'balanced']},\n",
    "    {'penalty': ['l2', 'none'], 'solver': ['newton-cg'], 'C': [100, 10, 5, 2, 1], 'class_weight': [{0: 1.2, 1: 1.0, 2: 2.0}, 'balanced']}\n",
    "]\n",
    "\n",
    "def precision_2(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[2]\n",
    "def recall_2(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[2]\n",
    "def multiclass_auc(y_true, y_prob):\n",
    "    return roc_auc_score(y_true, y_prob, multi_class='ovo', average='macro')\n",
    "auc_scorer = make_scorer(multiclass_auc, needs_proba=True, greater_is_better=True)  # multiclass_auc requires to accept y_prob as input, so we need to set needs_proba=True\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "precision_scorer = make_scorer(precision_2, greater_is_better=True)\n",
    "recall_scorer = make_scorer(recall_2, greater_is_better=True)\n",
    "scoring = {'AUC': auc_scorer, 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    LogisticRegression(tol=5e-3, max_iter=200, multi_class='multinomial', random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=15,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1', 'class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest + multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mul_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               criterion='gini',\n",
    "                               max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                               max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                               ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                               min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                               min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                               min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               max_features='sqrt', # Make sure trees are independent\n",
    "                               bootstrap=False, # Make sure trees are independent\n",
    "                               max_samples=None, # Make sure trees are independent\n",
    "                               oob_score=False,\n",
    "                               class_weight=None,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "# When boostrap is True, samples may not be equal to the sum of values in each node, as \"samples\" is the \"unique\" number of samples, but \"value\" is the number of \"repeated\" samples\n",
    "# https://stackoverflow.com/questions/56103507/why-does-this-decision-trees-values-at-each-step-not-sum-to-the-number-of-sampl\n",
    "plt.rcParams[\"figure.figsize\"] = (60, 60)\n",
    "tree.plot_tree(model.estimators_[0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [100, 200], \n",
    "     'criterion': ['gini', 'entropy'], \n",
    "     'max_depth': [3, 6], \n",
    "     'min_samples_split': [2, 10],\n",
    "     'min_samples_leaf': [1, 5],\n",
    "     'min_impurity_decrease': [0, 0.02],\n",
    "     'class_weight': [{0: 1.2, 1: 1.0, 2: 1.0}, 'balanced']}\n",
    "]\n",
    "\n",
    "def precision_2(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[2]\n",
    "def recall_2(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[2]\n",
    "def multiclass_auc(y_true, y_prob):\n",
    "    return roc_auc_score(y_true, y_prob, multi_class='ovo', average='macro')\n",
    "auc_scorer = make_scorer(multiclass_auc, needs_proba=True, greater_is_better=True)  # multiclass_auc requires to accept y_prob as input, so we need to set needs_proba=True\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "precision_scorer = make_scorer(precision_2, greater_is_better=True)\n",
    "recall_scorer = make_scorer(recall_2, greater_is_better=True)\n",
    "scoring = {'AUC': auc_scorer, 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1', 'class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT + multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mul_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = GradientBoostingClassifier(loss='deviance',  # binomial or multinomial deviance loss function\n",
    "                                   learning_rate=0.1, # Learning rate shrinks the contribution of each tree by learning_rate. The larger the learning rate is, the fewer estimators we need\n",
    "                                   n_estimators=100, # Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance\n",
    "                                   criterion='friedman_mse',\n",
    "                                   init='zero', # An estimator object that is used to compute the initial predictions\n",
    "                                   max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                                   max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                                   ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                                   min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                                   min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                                   min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                                   min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                                   subsample=1.0,  # The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias\n",
    "                                   max_features=None, # Make sure trees are independent\n",
    "                                   random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "tree.plot_tree(model.estimators_[0][0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest, where now model.estimators_ is an array of size n_estimators * loss_.K\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'learning_rate': [0.01, 0.1],\n",
    "     'n_estimators': [50, 100], \n",
    "     'max_depth': [3, 6], \n",
    "     'min_samples_split': [2, 10],\n",
    "     'min_samples_leaf': [1, 5],\n",
    "     'min_impurity_decrease': [0, 0.02], \n",
    "     'subsample': [1, 0.8]}\n",
    "]\n",
    "\n",
    "def precision_2(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[2]\n",
    "def recall_2(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[2]\n",
    "def multiclass_auc(y_true, y_prob):\n",
    "    return roc_auc_score(y_true, y_prob, multi_class='ovo', average='macro')\n",
    "auc_scorer = make_scorer(multiclass_auc, needs_proba=True, greater_is_better=True)  # multiclass_auc requires to accept y_prob as input, so we need to set needs_proba=True\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "precision_scorer = make_scorer(precision_2, greater_is_better=True)\n",
    "recall_scorer = make_scorer(recall_2, greater_is_better=True)\n",
    "scoring = {'AUC': auc_scorer, 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1', 'class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost + multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mul_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=feature_importances#xgboost.XGBClassifier\n",
    "# https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "        n_estimators=4,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.9,\n",
    "        objective='multi:softprob',\n",
    "        booster='gbtree',\n",
    "        tree_method='exact',\n",
    "        gamma=0.0,\n",
    "        min_child_weight=1.0,\n",
    "        max_delta_step=0.0,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.5,\n",
    "        random_state=0,\n",
    "        use_label_encoder=False,\n",
    "        missing=np.nan,\n",
    "        importance_type=None,\n",
    "        # num_class=y_train.nunique()  # You don't have to manually specify the number of classes here\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "If \"balanced\", class weight for class i = (n_total_samples / n_classes) * (1 / n_samples_of_class_i)\n",
    "If a dictionary is given, keys are classes and values are corresponding class weights\n",
    "If None is given, the class weights will be uniform\n",
    "\"\"\"\n",
    "weights = compute_class_weight('balanced', classes=y_train.unique(), y=y_train)\n",
    "dict_weights = dict(zip(y_train.unique(), weights))\n",
    "print(dict_weights)\n",
    "\n",
    "model.fit(x_train, y_train, eval_metric='merror', sample_weight=y_train.replace(dict_weights))\n",
    "\n",
    "print(model.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [5, 10, 15, 20], \n",
    "     'learning_rate': [0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "     'max_depth': [3, 6, 9, 12], \n",
    "     'gamma': [0.0, 0.5, 1.0],\n",
    "     'min_child_weight': [1.0, 2.0],\n",
    "     'max_delta_step': [0.0, 0.5], \n",
    "     'lambda': [0.0, 0.5, 1.0], \n",
    "     'alpha': [0.0, 0.5, 1.0], \n",
    "    }\n",
    "]\n",
    "\n",
    "def precision_2(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[2]\n",
    "def recall_2(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[2]\n",
    "def multiclass_auc(y_true, y_prob):\n",
    "    return roc_auc_score(y_true, y_prob, multi_class='ovo', average='macro')\n",
    "auc_scorer = make_scorer(multiclass_auc, needs_proba=True, greater_is_better=True)  # multiclass_auc requires to accept y_prob as input, so we need to set needs_proba=True\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "precision_scorer = make_scorer(precision_2, greater_is_better=True)\n",
    "recall_scorer = make_scorer(recall_2, greater_is_better=True)\n",
    "scoring = {'AUC': auc_scorer, 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    xgb.XGBClassifier(use_label_encoder=False, random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "weights = compute_class_weight('balanced', classes=y_train.unique(), y=y_train)\n",
    "dict_weights = dict(zip(y_train.unique(), weights))\n",
    "rs.fit(x_train, y_train, eval_metric='merror', sample_weight=y_train.replace(dict_weights))\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1', 'class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP + multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mul_clf_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(30, 20, 10),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size=256,\n",
    "    learning_rate='constant', # Only used when solver='sgd'\n",
    "    learning_rate_init=0.001, # Only used when solver='sgd' or 'adam'\n",
    "    power_t=0.5, # It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'\n",
    "    max_iter=300, # For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps\n",
    "    shuffle=True, # Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'\n",
    "    random_state=0,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1 # The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "\n",
    "print(model.n_layers_)\n",
    "print(model.n_outputs_)\n",
    "print(model.out_activation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loss = pd.DataFrame({'n_epoch': list(range(1, len(model.loss_curve_) + 1)), 'train_loss': model.loss_curve_})\n",
    "sns.lineplot(x='n_epoch', y='train_loss', data=df_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'hidden_layer_sizes': list(itertools.product(*[[40, 30], [30, 20], [20, 10, 5]])), \n",
    "     'alpha': [0.001, 0.0001], \n",
    "     'batch_size': [512, 256], \n",
    "     'learning_rate_init': [0.01, 0.001]},\n",
    "]\n",
    "\n",
    "def precision_2(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[2]\n",
    "def recall_2(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[2]\n",
    "def multiclass_auc(y_true, y_prob):\n",
    "    return roc_auc_score(y_true, y_prob, multi_class='ovo', average='macro')\n",
    "auc_scorer = make_scorer(multiclass_auc, needs_proba=True, greater_is_better=True)  # multiclass_auc requires to accept y_prob as input, so we need to set needs_proba=True\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "precision_scorer = make_scorer(precision_2, greater_is_better=True)\n",
    "recall_scorer = make_scorer(recall_2, greater_is_better=True)\n",
    "scoring = {'AUC': auc_scorer, 'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    MLPClassifier(max_iter=300, random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    scoring=scoring,\n",
    "    refit='AUC',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_AUC', 'std_test_AUC', 'mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_AUC', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1', 'class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC + multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'mul_clf_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It uses either 'ovo' or 'ovr' to adapt SVC to multi-classification, which can be specified by \"decision_function_shape\"\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/svm.html#multi-class-classification\n",
    "ovo: https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\n",
    "ovr: https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "\"\"\"\n",
    "\n",
    "model = SVC(C=1.0, \n",
    "            kernel='rbf',\n",
    "            degree=3, # Degree of the polynomial kernel function ('poly'). Ignored by all other kernels\n",
    "            gamma='auto', # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'; if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma; if 'auto', uses 1 / n_features\n",
    "            coef0=0.0, # Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'\n",
    "            shrinking=True, \n",
    "            probability=False, # Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation\n",
    "            class_weight='balanced',\n",
    "            random_state=0, \n",
    "            decision_function_shape='ovr').fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [100, 10, 1], \n",
    "     'kernel': ['linear', 'rbf', 'poly'], \n",
    "     'class_weight': [{0: 1.2, 1: 1.0, 2: 2.0}, 'balanced']},\n",
    "]\n",
    "\n",
    "# Can't use \"auc_scorer\" as we did above, as SVC can't give probability\n",
    "def precision_2(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[2]\n",
    "def recall_2(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[2]\n",
    "accuracy_scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "precision_scorer = make_scorer(precision_2, greater_is_better=True)\n",
    "recall_scorer = make_scorer(recall_2, greater_is_better=True)\n",
    "scoring = {'Accuracy': accuracy_scorer, 'Precision': precision_scorer, 'Recall': recall_scorer}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    SVC(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    scoring=scoring,\n",
    "    refit='Accuracy',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_Accuracy', 'std_test_Accuracy', 'mean_test_Precision', 'std_test_Precision', 'mean_test_Recall', 'std_test_Recall']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba_pred = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "acc_test = model.score(x_test, y_test)\n",
    "\n",
    "# assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1', 'class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model + regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'reg_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze() / 100000  # Dividing it by 100000 will significantly improve the following result\n",
    "y_test = y_test.squeeze() / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "\"\"\"\n",
    "a * ||w||_1 + 0.5 * b * ||w||_2^2\n",
    "alpha = a + b and l1_ratio = a / (a + b)\n",
    "\"\"\"\n",
    "model = ElasticNet(alpha=1, l1_ratio=0.5, fit_intercept=True, \n",
    "                   random_state=0, tol=1e-4, max_iter=1000).fit(x_train, y_train)  \n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'alpha': list(np.arange(0.001, 1, 0.05)), 'l1_ratio': list(np.arange(0, 1, 0.01))},  # In my case, alpha should not be too small, otherwise the matrix is singular and the optimization algorithm doesn't converge\n",
    "]\n",
    "\n",
    "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False)}  # As you can see in df_cv, make_scorer will multiply -1 to MSE \n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    ElasticNet(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=2000,\n",
    "    scoring=scoring,\n",
    "    refit='MSE',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_MSE', 'std_test_MSE']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_MSE'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "df_predict = pd.concat((y_test, pd.Series(y_pred)), axis=1)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest + regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'reg_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze() / 100000  # Dividing it by 100000 will significantly improve the following result\n",
    "y_test = y_test.squeeze() / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = RandomForestRegressor(n_estimators=100, \n",
    "                               criterion='mse',\n",
    "                               max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                               max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                               ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                               min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                               min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                               min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               max_features=1 / 3, # Make sure trees are independent\n",
    "                               bootstrap=True, # Make sure trees are independent\n",
    "                               max_samples=None, # Make sure trees are independent\n",
    "                               oob_score=True,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "# When boostrap is True, samples may not be equal to the sum of values in each node, as \"samples\" is the \"unique\" number of samples, but \"value\" is the number of \"repeated\" samples\n",
    "# https://stackoverflow.com/questions/56103507/why-does-this-decision-trees-values-at-each-step-not-sum-to-the-number-of-sampl\n",
    "plt.rcParams[\"figure.figsize\"] = (60, 60)\n",
    "tree.plot_tree(model.estimators_[0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [100, 200], \n",
    "     'criterion': ['mse'], # Using 'mae' here will make the following CV very slow, I don't know why\n",
    "     'max_depth': [3, 6], \n",
    "     'min_samples_split': [2, 10],\n",
    "     'min_samples_leaf': [1, 5],\n",
    "     'min_impurity_decrease': [0, 0.02]}\n",
    "]\n",
    "\n",
    "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False)}  # As you can see in df_cv, make_scorer will multiply -1 to MSE \n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=0, max_features=1 / 3),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='MSE',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_MSE', 'std_test_MSE']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_MSE'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "df_predict = pd.concat((y_test, pd.Series(y_pred)), axis=1)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT + regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'reg_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze() / 100000  # Dividing it by 100000 will significantly improve the following result\n",
    "y_test = y_test.squeeze() / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = GradientBoostingRegressor(loss='ls', \n",
    "                                  learning_rate=0.1, # Learning rate shrinks the contribution of each tree by learning_rate. The larger the learning rate is, the fewer estimators we need\n",
    "                                  n_estimators=100, # Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance\n",
    "                                  criterion='friedman_mse',\n",
    "                                  init='zero', # An estimator object that is used to compute the initial predictions\n",
    "                                  max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                                  max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                                  ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                                  min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                                  min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                                  min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                                  min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                                  subsample=1.0,  # The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias\n",
    "                                  max_features=None, # Make sure trees are independent\n",
    "                                  random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "tree.plot_tree(model.estimators_[0][0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest, where now model.estimators_ is an array of size n_estimators * loss_.K\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'learning_rate': [0.01, 0.1],\n",
    "     'n_estimators': [50, 100], \n",
    "     'max_depth': [3, 6], \n",
    "     'min_samples_split': [2, 10],\n",
    "     'min_samples_leaf': [1, 5],\n",
    "     'min_impurity_decrease': [0, 0.02], \n",
    "     'subsample': [1, 0.8]}\n",
    "]\n",
    "\n",
    "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False)}  # As you can see in df_cv, make_scorer will multiply -1 to MSE \n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='MSE',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_MSE', 'std_test_MSE']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_MSE'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "df_predict = pd.concat((y_test, pd.Series(y_pred)), axis=1)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost + regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'reg_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze() / 100000  # Dividing it by 100000 will significantly improve the following result\n",
    "y_test = y_test.squeeze() / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=feature_importances#xgboost.XGBClassifier\n",
    "# https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "        n_estimators=4,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.9,\n",
    "        objective='reg:squarederror',\n",
    "        booster='gbtree',\n",
    "        tree_method='exact',\n",
    "        gamma=0.0,\n",
    "        min_child_weight=1.0,\n",
    "        max_delta_step=0.0,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.5,\n",
    "        random_state=0,\n",
    "        use_label_encoder=False,\n",
    "        missing=np.nan,\n",
    "        importance_type=None,\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, eval_metric='rmse')\n",
    "\n",
    "print(model.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [5, 10, 15, 20], \n",
    "     'learning_rate': [0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "     'max_depth': [3, 6, 9, 12], \n",
    "     'gamma': [0.0, 0.5, 1.0],\n",
    "     'min_child_weight': [1.0, 2.0],\n",
    "     'max_delta_step': [0.0, 0.5], \n",
    "     'lambda': [0.0, 0.5, 1.0], \n",
    "     'alpha': [0.0, 0.5, 1.0], \n",
    "    }\n",
    "]\n",
    "\n",
    "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False)}  # As you can see in df_cv, make_scorer will multiply -1 to MSE \n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(use_label_encoder=False, random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=scoring,\n",
    "    refit='MSE',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train, eval_metric='rmse')\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_MSE', 'std_test_MSE']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_MSE'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "df_predict = pd.concat((y_test, pd.Series(y_pred)), axis=1)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP + regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'reg_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze() / 100000  # Dividing it by 100000 will significantly improve the following result\n",
    "y_test = y_test.squeeze() / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(30, 20, 10),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size=256,\n",
    "    learning_rate='constant', # Only used when solver='sgd'\n",
    "    learning_rate_init=0.0001, # Only used when solver='sgd' or 'adam'\n",
    "    power_t=0.5, # It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'\n",
    "    max_iter=300, # For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps\n",
    "    shuffle=True, # Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'\n",
    "    random_state=0,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1 # The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "\n",
    "print(model.n_layers_)\n",
    "print(model.n_outputs_)\n",
    "print(model.out_activation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loss = pd.DataFrame({'n_epoch': list(range(1, len(model.loss_curve_) + 1)), 'train_loss': model.loss_curve_})\n",
    "sns.lineplot(x='n_epoch', y='train_loss', data=df_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "param_grid = [\n",
    "    {'hidden_layer_sizes': list(itertools.product(*[[40, 30], [30, 20], [20, 10, 5]])), \n",
    "     'alpha': [0.001, 0.0001], \n",
    "     'batch_size': [512, 256], \n",
    "     'learning_rate_init': [0.01, 0.001]},\n",
    "]\n",
    "\n",
    "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False)}  # As you can see in df_cv, make_scorer will multiply -1 to MSE \n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    MLPRegressor(max_iter=300, random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,\n",
    "    scoring=scoring,\n",
    "    refit='MSE',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_MSE', 'std_test_MSE']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_MSE'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "df_predict = pd.concat((y_test, pd.Series(y_pred)), axis=1)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR + regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'reg_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze() / 100000  # Dividing it by 100000 will significantly improve the following result\n",
    "y_test = y_test.squeeze() / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(C=1.0, \n",
    "            kernel='rbf',\n",
    "            degree=3, # Degree of the polynomial kernel function ('poly'). Ignored by all other kernels\n",
    "            gamma='auto', # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'; if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma; if 'auto', uses 1 / n_features\n",
    "            coef0=0.0, # Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'\n",
    "            epsilon=0.1, # Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value\n",
    "            shrinking=True).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and search hyperparameters by CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "# rank_test_score indicates the rank of a grid search parameter combination based on the mean_test_score\n",
    "\n",
    "# This cell is too slow to run!\n",
    "param_grid = [\n",
    "    {'C': [100, 10, 1], \n",
    "     'kernel': ['linear', 'rbf', 'poly']},\n",
    "]\n",
    "\n",
    "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False)}  # As you can see in df_cv, make_scorer will multiply -1 to MSE \n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    SVR(),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=2,\n",
    "    scoring=scoring,\n",
    "    refit='MSE',\n",
    "    return_train_score=False,\n",
    "    cv=5, n_jobs=-1, random_state=0\n",
    ")\n",
    "rs.fit(x_train, y_train)\n",
    "\n",
    "model = rs.best_estimator_\n",
    "print(rs.best_estimator_)\n",
    "print(rs.best_params_)\n",
    "\n",
    "result = rs.cv_results_\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(result['params'])\n",
    "df_metrics = pd.DataFrame({k: result[k] for k in ['mean_test_MSE', 'std_test_MSE']})\n",
    "df_cv = pd.concat((df_params, df_metrics), axis=1).sort_values(by=['mean_test_MSE'], ascending=False)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "df_predict = pd.concat((y_test, pd.Series(y_pred)), axis=1)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination (use random forest + binary classification as example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'bin_clf_no_onehot'\n",
    "x_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_train.csv'.format(folder), sep=',', index_col=False)\n",
    "x_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/x_test.csv'.format(folder), sep=',', index_col=False)\n",
    "y_train = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_train.csv'.format(folder), sep=',', index_col=False)\n",
    "y_test = pd.read_csv('/Users/YouranQi/Documents/Career/ds_and_ml_interview/template/{:}/y_test.csv'.format(folder), sep=',', index_col=False)\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model once and get importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model once\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               criterion='gini',\n",
    "                               max_depth=3, # Avoid overfitting by controling the complexity of the tree\n",
    "                               max_leaf_nodes=None, # Avoid overfitting by controling the complexity of the tree\n",
    "                               ccp_alpha=0.0, # Avoid overfitting by pruning the tree\n",
    "                               min_samples_split=2, # Avoid overfitting by avoid splitting too much\n",
    "                               min_samples_leaf=1, # Avoid overfitting by avoid splitting too much\n",
    "                               min_weight_fraction_leaf=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               min_impurity_decrease=0.0, # Avoid overfitting by avoid splitting too much\n",
    "                               max_features='sqrt', # Make sure trees are independent\n",
    "                               bootstrap=True, # Make sure trees are independent\n",
    "                               max_samples=None, # Make sure trees are independent\n",
    "                               oob_score=True,\n",
    "                               class_weight='balanced',\n",
    "                               n_jobs=-1,\n",
    "                               random_state=0).fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "tree.plot_tree(model.estimators_[0], feature_names=x_train.columns)  # Visualize one of the trees in the random forest\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importance = pd.concat((pd.Series(x_train.columns), pd.Series(model.feature_importances_)), axis=1)\n",
    "df_importance.columns = ['feature', 'importance_score']\n",
    "df_importance = df_importance.sort_values(by='importance_score', ascending=False)\n",
    "df_importance.plot(x='feature', y='importance_score', kind='bar', figsize=(20, 10))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we didn't do hyperparameter search\n",
    "# In practice, you should follow the steps mentioned at https://stats.stackexchange.com/a/323899\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, pos_label=1, average='binary')\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1, average='binary')\n",
    "\n",
    "all_n_features_selected = list(reversed(range(1, df_importance.shape[0] + 1)))\n",
    "all_scores = []\n",
    "for n_features_selected in all_n_features_selected:\n",
    "    features_selected = df_importance['feature'].iloc[:n_features_selected].tolist()\n",
    "    scores = cross_val_score(model, \n",
    "                             x_train.loc[:, features_selected], \n",
    "                             y_train, \n",
    "                             scoring='roc_auc',  # can be 'roc_auc', accuracy_scorer, precision_scorer, recall_scorer\n",
    "                             cv=5, n_jobs=-1, fit_params=None)\n",
    "    all_scores.append(sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfe = pd.DataFrame({'n_features': all_n_features_selected, 'scores': all_scores})\n",
    "df_rfe.plot(x='n_features', y='scores', kind='line', figsize=(20, 10))\n",
    "df_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_selected = df_importance['feature'].iloc[:12].tolist()\n",
    "x_train_selected = x_train.loc[:, features_selected]\n",
    "x_test_selected = x_test.loc[:, features_selected]\n",
    "\n",
    "model = model.fit(x_train_selected, y_train)\n",
    "\n",
    "print(features_selected)\n",
    "print(model.score(x_train_selected, y_train))\n",
    "tree.plot_tree(model.estimators_[0], feature_names=x_train_selected.columns)  # Visualize one of the trees in the random forest\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = model.predict_proba(x_test_selected)\n",
    "y_pred = model.predict(x_test_selected)\n",
    "acc_test = model.score(x_test_selected, y_test)\n",
    "\n",
    "assert (proba_pred.argmax(axis=1) == y_pred).all()\n",
    "assert acc_test == accuracy_score(y_test, y_pred) == (y_test == pd.Series(y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_metrics(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(y_test, y_pred, n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, x_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(model, x_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about your final conclusions and business insights, depending on the goal of this data analysis.\n",
    "\n",
    "You may say you tried several models, and their results are consistent: all models think several particular variables are important (according to which variables a linear model selects or the importance scores of a tree model)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
